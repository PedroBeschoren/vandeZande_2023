---
title: "3_time_serias_mia_package"
author: "Pedro Beschoren da Costa"
date: "2023-08-14"
output: html_document
editor_options: 
  chunk_output_type: console
---


# load
```{r}

library(phyloseq)
library(vegan)
library(dplyr)
library(ggplot2)
library(metagMisc)
library(ggpubr)
library(plyr)
library(dplyr)
library(Boruta)
library(tibble)
library(metacoder)
library(caret)
library(ranger)


source("./Code/Machine_Learning_custom_functions.R") # source


load(file = "./Data/physeq_rarefied_soiltype_l.RData")




```

# RF for for time
```{r}
# rename object to avoid copy-paste previosu code issues
ps_sp_l<-physeq_rarefied_soiltype_l

# find ASVs taht differentiate the time points
t0<-Sys.time()
set.seed(101)
Boruta_time_l<-lapply(ps_sp_l, function (x) #parLapply wil do a parallel lapply on the defined cluster
  Boruta(Time_point~.,   # classification you are trying to predict
         data = single_physeq_to_borutaInput(physeq_object =x,
                                             variable_to_be_classified = "Time_point")[,-1], # removes first column, "sample" as a predictor variable
         doTrace=0,
         maxRuns = 200,  #increase the maximum number of runs to decrease the number of tenttively important OTUs.
         ntree = 10000)) # increase the number of trees to increase precision. decrease ntree/maxruns to reduce computational time.
t1<-Sys.time()
t0-t1

save(Boruta_time_l, file = "./Results/Boruta_time_l.RData")
load(file = "./Results/Boruta_time_l.RData")
         
         


```


# RF for treatment
```{r}


# find ASVs taht differentiate the time points
t0<-Sys.time()
set.seed(101)
Boruta_Treatment_l<-lapply(ps_sp_l, function (x) #parLapply wil do a parallel lapply on the defined cluster
  Boruta(Treatment~.,   # classification you are trying to predict
         data = single_physeq_to_borutaInput(physeq_object =x,
                                             variable_to_be_classified = "Treatment")[,-1], # removes first column, "sample" as a predictor variable
         doTrace=0,
         maxRuns = 200,  #increase the maximum number of runs to decrease the number of tenttively important OTUs.
         ntree = 10000)) # increase the number of trees to increase precision. decrease ntree/maxruns to reduce computational time.
t1<-Sys.time()
t0-t1

save(Boruta_Treatment_l, file = "./Results/Boruta_Treatment_l.RData")
load(file = "./Results/Boruta_Treatment_l.RData")
         


```




#fix boruta, save as PS
```{r}

#put both boruta objects into a single list
Boruta_l<-c(Boruta_time_l,Boruta_Treatment_l)
names(Boruta_l)<-c("Time_Field", "Time_pot", "Treatment_field", "Treatment_pot")

#fixed boruta
fixed_Boruta_l<-lapply(Boruta_l, TentativeRoughFix)

#RF aSVs
fixed_Boruta_ASV_l<-lapply(fixed_Boruta_l, getSelectedAttributes)

# ps with RF ASVs
ps_RF_ASVs<-
  c(prune_taxa(taxa = fixed_Boruta_ASV_l$Time_Field,
           x = physeq_rarefied_soiltype_l$Field),
    prune_taxa(taxa = fixed_Boruta_ASV_l$Time_pot,
           x = physeq_rarefied_soiltype_l$Pot),
    prune_taxa(taxa = fixed_Boruta_ASV_l$Treatment_field,
           x = physeq_rarefied_soiltype_l$Field),
    prune_taxa(taxa = fixed_Boruta_ASV_l$Treatment_pot,
           x = physeq_rarefied_soiltype_l$Pot))

#adjsut names
names(ps_RF_ASVs)<-names(fixed_Boruta_ASV_l)




```




```{r}
# define function for plotting:
phyloseq_to_heat_tree<-function(ps_object, sample_group){
  
  # this function output is a heat tree comparing metadata. the input is based on a phyloseq object
  
  # ps object =  a phyloseq object, containing sample metadata, OTU table, and taxonomy table
  # sample_group = the name of a column in your metadata that you want to compare in the heat tree. it has to be quoted.
  
  # this function will create a matrix of heat trees if your metadata has more than 2 groups. it should fail if it has only 1 group
  
  
#exchange species info per ASV ID
ps_object@tax_table[,7]<-taxa_names(ps_object)

# turn NA into "" so it is not completety missing ofmr the tree  
ps_object@tax_table@.Data[is.na(ps_object@tax_table)]<-""


#remove unnecessary taxonomic info (dada2id, "s__" and "ASV_id) by updating the tax table with a subset of the tax table
tax_table(ps_object)<-tax_table(ps_object)[,1:7]


# let's remove the "r__"ranks from the taxonomy, they can be useful but will pollute our plot
tax_table(ps_object)[, colnames(tax_table(ps_object))] <- gsub(pattern = "[a-z]__", # regular expression pattern to search for
                                                                   x = tax_table(ps_object)[, colnames(tax_table(ps_object))], # "df"
                                                                   replacement = "") # replacement for pattern
# transform from phyloseq to taxmap object
taxmap_obj<-parse_phyloseq(ps_object)

#get abundance per taxon
taxmap_obj$data$tax_abund<-calc_taxon_abund(obj = taxmap_obj, 
                                      data = "otu_table",
                                      cols = taxmap_obj$data$sample_data$sample_id) 

#get occurrence of ASVs per treatment
# the sample groups needs some wrangling to fit within the soft code of the function
taxmap_obj$data$tax_occ<- calc_n_samples(obj = taxmap_obj, 
                                                      data = "tax_abund", 
                                                      cols = taxmap_obj$data$sample_data$sample_id,
                                                      groups = taxmap_obj$data$sample_data[colnames(taxmap_obj$data$sample_data)==sample_group][[1]]) 

# calculate log2 median ratios and p values for a wilcoxcon test within taxas in this stress treatment groups
# the sample groups needs some wrangling to fit within the soft code of the function
taxmap_obj$data$diff_table <- compare_groups(obj = taxmap_obj,
                                                        data = "tax_abund",
                                                        cols = taxmap_obj$data$sample_data$sample_id, 
                                                        groups = taxmap_obj$data$sample_data[colnames(taxmap_obj$data$sample_data)==sample_group][[1]]) 


# define number of compared factors
factors_compared<-taxmap_obj$data$sample_data[colnames(taxmap_obj$data$sample_data)==sample_group][[1]] 

# draw the plot based on an if else statement: if there are 2 groups, plot a a heat tree comparing abundances between both groups, else plot a matrix of ehat trees. this function will fail if you only have 1 sample group! 

if (length(unique(factors_compared)) == 2) {

set.seed(1)
 output<- taxmap_obj %>%
heat_tree(
            node_label = taxon_names,
            node_size = n_obs,
            node_color = log2_median_ratio,
            node_color_interval = c(-3, 3), # The range of `log2_median_ratio` to display
            node_color_range = c("cyan", "gray", "tan"), # The color palette used
            layout = "davidson-harel",
            node_color_axis_label = paste0("log2 median; tan", 
                                           taxmap_obj$data$diff_table$treatment_1[1], 
                                           "cyan", 
                                           taxmap_obj$data$diff_table$treatment_2[1],
                                           sep = " "),
            initial_layout = "reingold-tilford")

 } else {

set.seed(1)
output<-heat_tree_matrix(taxmap_obj,
                         data = "diff_table", # this is the table with the data you want to plot
                         node_size = n_obs, # n_obs is a function that calculates, in this case, the number of OTUs per taxon
                         node_label = taxon_names,
                         node_color = log2_median_ratio, # A column from `taxmap_obj$data$diff_table_3treatments`
                         node_color_range = diverging_palette(), # The built-in palette for diverging data
                         node_color_interval = c(-3, 3), # The range of `log2_median_ratio` to display
                         edge_color_interval = c(-3, 3), # The range of `log2_median_ratio` to display
                         node_size_axis_label = "Number of OTUs",
                         node_color_axis_label = "Log2 ratio median proportions",
                         layout = "davidson-harel", # The primary layout algorithm
                         initial_layout = "reingold-tilford") # The layout algorithm that initializes node locations

   
   
 }


# clearly define the output object you will get from the function
return(output)   


}



# rearrange order of samples so the groups are identical across the 3 heat trees
ps_RF_ASVs<-
lapply(ps_RF_ASVs, function(x){
  
 # sample_names_sorted<-sort(sample_names(x))
  
string_to_sort<-  sample_names(x)
  # Create a custom order
custom_order <- c("Co", "Man", "Chi", "BSF", "HC", "MW")

# Extract the substring that you want to sort by (e.g., the part before the first dot)
substring_to_sort_by <- sub("^(.*?)\\..*", "\\1", string_to_sort)

# Use the custom order for sorting
sorted_strings <- string_to_sort[order(match(substring_to_sort_by, custom_order))]

#reorder sample anmes according enw isntruction
  otu_table(x) <- otu_table(x)[, sorted_strings]

  return(x)
  
})



#creat heat trees
heat_tree_time<-
  lapply(ps_RF_ASVs[1:2], function(x)
    phyloseq_to_heat_tree(ps_object = x, sample_group = "Time_point"))


heat_tree_treatment<-
  lapply(ps_RF_ASVs[3:4], function(x)
    phyloseq_to_heat_tree(ps_object = x, sample_group = "Treatment"))

#export as pdf
ggsave(plot = heat_tree_time$Time_Field, filename = "./Results/RF_Heattree_Time_Field.pdf")
ggsave(plot = heat_tree_time$Time_pot, filename = "./Results/RF_Heattree_Time_pot.pdf")
ggsave(plot = heat_tree_treatment$Treatment_field, filename = "./Results/RF_Heattree_Treatment_Field.pdf")
ggsave(plot = heat_tree_treatment$Treatment_pot, filename = "./Results/RF_Heattree_Treatment_pot.pdf")


```



# explore taxonomy and importance of features
```{r}

#df with ASV importance
RF_importance_df_l<-
lapply(fixed_Boruta_l, function(x)
  dplyr::filter(attStats(x), decision == "Confirmed"))


# define a function to extract  mean importance and linear biomass correlations of each taxonomy of the ASVs selected by the random forest
Importance_per_taxon<-function(ps, ASV_stats_boruta){
  
  confirmed_ASV<-rownames(ASV_stats_boruta)
  
imp_ps<-prune_taxa(taxa = confirmed_ASV, x = ps) 

imp_melted<-psmelt(imp_ps)

to_merge<-rownames_to_column(ASV_stats_boruta[,1:2], var = "OTU")

imp_melted<-left_join(imp_melted, to_merge, by ="OTU")

phylum_table<-imp_melted%>%
  group_by(Phylum)%>%
  dplyr::summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

class_table<-imp_melted%>%
  group_by(Class)%>%
  dplyr::summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

order_table<-imp_melted%>%
  group_by(Order)%>%
  dplyr::summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

           
family_table<-imp_melted%>%
  group_by(Family)%>%
  dplyr::summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

genus_table<-imp_melted%>%
  group_by(Genus)%>%
  dplyr::summarize(mean_imp = mean(meanImp),
            imp_sd = sd(meanImp),
            n=n()/nsamples(ps))%>%
  dplyr::arrange(desc(n))%>%as.data.frame()

output<-list("phylum_table" = phylum_table,
             "class_table" = class_table,
             "order_table" = order_table,
             "family_table" = family_table,
             "genus_table" = genus_table)
return(output)

}

# execute custom function on list of boruta and ps obejcts
importance_per_taxonomy<-mapply(function(x,y)
  Importance_per_taxon(ps = x,ASV_stats_boruta = y),
  x = ps_RF_ASVs,
  y = RF_importance_df_l,
  SIMPLIFY = FALSE)


#export this table as txt
capture.output(importance_per_taxonomy, file = "./Results/RF_importance_per_taxonomy.txt")

```

# define a few functions to test model
```{r}
#prepare input to boruta
single_physeq_to_borutaInput<-function (physeq_object, variable_to_be_classified){
  # boruta expects a transposed OTU table with variable_to_be_classified as a added variable
  # the output is a list of df ready to be used as input to boruta  
  #transpose phtseq otu table  
  otu_cells_wide_list <- #transpose the feature table...
    base::as.data.frame(t(otu_table(physeq_object)))%>%
      rownames_to_column(var = "sample")
  
  # extract sample data
  metadata_list <-
    as(sample_data(physeq_object),"data.frame")%>%
      rownames_to_column(var = "sample")
  
  #add the variable classification you want to predict with the random forest
  boruta_dataset<-
    base::merge(dplyr::select(metadata_list,sample, variable_to_be_classified),
                otu_cells_wide_list,
                by = "sample",
                all.y = TRUE)
  
  #make sure your variable to be classified is a numeic, or boruta won't run
  
    boruta_dataset[,2]<-as.numeric(boruta_dataset[,2]) # saves the second column, your variable_to_be_classified, as numeric
   
    output<-boruta_dataset
    
  gc()
  return(output)
}

# write a function to split training and test data from a phyloseq object
train_and_test_spliter<-function(ps_object, variable_to_be_classified){
  
  # this function will separate train and test sets based on a phyloseq object and the variable to be predicted. it requires the function single_physeq_to_borutaInput()
  # ps_object = a phyloseq object
  # variable_to_be_classified =  a (quoted) metadata column that you want to predict
  # the output is a list of two objects: the first is the training set, the second is the test set
  
  # wrangle phyloseq data
  ps_data<-single_physeq_to_borutaInput(physeq_object = ps_object,
                                        variable_to_be_classified = variable_to_be_classified)
  
  # define training and test set. this can be ofptimized for repeated k-fold cross validation
  trainIndex<- createDataPartition(ps_data[,2], 
                                   p = .70, 
                                   list = FALSE, 
                                   times = 1)
  # set train and test sets
  data_Train <- ps_data [ trainIndex,]
  data_Test  <- ps_data [-trainIndex,]
  
  output<-list(data_Train,data_Test)
  names(output)<-c("data_Train","data_Test")
  
  return(output)
  
}

# define a function to fix borta objects and put them into formula format
fixed_boruta_formula<-function(boruta_object){
  # this fucntion takes a boruta ofbect, fixes the inconclusive tas into importnat o unimportnat, and then generates a formula
  # the input is a boruta object
  # the output is a boruta formula to be fed to caret::train
  # NOTE: boruta objects with zero imporntat features may crash!
  
  fixed_boruta<-TentativeRoughFix(boruta_object)
  boruta_imp_ASV<-getSelectedAttributes(fixed_boruta)
  print("number of importnat ASVs. Warning: if zero, formula will crash!")
  print(length(boruta_imp_ASV)%>%unlist()%>%sort())
  formula_boruta<-getConfirmedFormula(fixed_boruta)
  
  return(formula_boruta)
}

# put fixing, spliting, training and testing all in a single function
fix_split_train<-function (boruta_output_l, ps_object_l, variable_to_be_classified){
  # this fucntion will fix tentative features in a list of boruta objects
  # then it will split a list of phyloseq objects into training and test sets
  # then it will train the list of models
  # then it will test the lsit of models
  # then it returns a list of confusion matrixes (one for each model)
    # boruta_output_l = a list of boruta objects
    # ps_object_l = a list of phyloseq objects
    # variable_to_be_classified = the metadata varaible you are trying to predict (must be quoted, like "Stress")
  
  # fix boruta in a formula to be evaluated with caret
  boruta_formula_bac_l<-lapply(boruta_output_l, function(x) fixed_boruta_formula(x))
  
  # split train adn test dataset
  train_test_l<-lapply(ps_object_l, function (x)
    train_and_test_spliter(ps_object = x, 
                           variable_to_be_classified = variable_to_be_classified))
  
  
  
  # train model
  boruta_feature_rf_repeatedcv<-mapply(function (x,z) {
    
    train.control <- caret::trainControl(method = "repeatedcv", # set trainig/data split controls for the train function
                                         number = 5,
                                         repeats = 10,
                                         allowParallel = TRUE)
    
    model_borutized <- caret::train(form = z, # bruta formula
                                    data = x[[1]], # training data ; first element of train_and_test_spliter()
                                    method = "rf", #execute training based on RF
                                    trControl = train.control, # defined in trainControl() above
                                    ntree=600,
                                    tuneLength = 5  )
    
    
    
    return(model_borutized)
  },
  x = train_test_l,
  z = boruta_formula_bac_l,
  SIMPLIFY = FALSE)
  
  
  output<-list("train_test_l" = train_test_l,
               "boruta_feature_rf_repeatedcv" = boruta_feature_rf_repeatedcv)
  
  
  return(output)
  
  
}

# define function to extract some key model metrics after CV precision testing
extract_confusionmatrix_metrics<-function(CV_output_l, Imp_ASV_stats_l){
  # this fucntion will extract key RF model metris from lists of RF models
  # CV_output_l = list of model testing objects from fix_split_train_test() custom function
  # Imp_ASV_stats_l = list of important ASV stats from a fixed boruta object
  # the output is a df with metrics for each model

  # accurayc, kappa, AccuracyPValue  
  
  cv_metrics<-map(CV_output_l,3)
  
  accuracy<-unlist(map(cv_metrics,1)) #accuracy
  
  kappa<-unlist(map(cv_metrics,2)) #kappa
  AccuracyLower<-unlist(map(cv_metrics,3)) #AccuracyLower  
  AccuracyUpper<-unlist(map(cv_metrics,4)) #AccuracyUpper  
  AccuracyPValue<-unlist(map(cv_metrics,6)) #AccuracyPValue 
  
  
  
  model_metrics<-as.data.frame(accuracy)
  model_metrics$kappa<-kappa     
  model_metrics$AccuracyLower<-AccuracyLower  
  model_metrics$AccuracyUpper<-AccuracyUpper  
  model_metrics$AccuracyPValue<-AccuracyPValue
  
  # n important stress predictor ASVs  
  model_metrics$n_imp_ASVs<-lapply(Imp_ASV_stats_l, function (x) length(x[,1]))%>%unlist
  
  # mean average imporance of ASVs in model
  model_metrics$mean_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) mean(x[,2]))%>%unlist
  
  # mean median imporance of ASVs in model
  model_metrics$median_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) mean(x[,3]))%>%unlist
  
  # sd imporance of ASVs in model
  model_metrics$sd_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) sd(x[,2]))%>%unlist
  
  # men normHits of ASVs in model
  model_metrics$mean_normHits_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) mean(x[,6]))%>%unlist
  
  # sd normHits of ASVs in model
  model_metrics$sd_normHits_ASV_imp<-lapply(Imp_ASV_stats_l, function (x) sd(x[,6]))%>%unlist
  
  return(model_metrics)
}

```


#check performance of the models
```{r}


set.seed(23456)
replicated_model_treatment<-replicate(15, 
                                      fix_split_train_test_replicated(boruta_output_l = Boruta_Treatment_l,
                                                                      ps_object_l = ps_sp_l,
                                                                      variable_to_be_classified ="Treatment"))

save(replicated_model_treatment, file = "./Data/replicated_model_treatment.RData")


```



